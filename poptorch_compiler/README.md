## PopTorch Compiler

Poptorch compiler is our new experimental backend which is designed to be PyTorch native from the get go. This means it has to support native PyTorch semantics directly, such as inplacing, arbitrary gradients and backward passes, etc.

It uses MLIR to create an IR which represents high level PyTorch operators and provides a PopLibs implementation of those operations. Eventually this will lower directly to PopIR, either via snap or by a mlir conversion pass.

The folder breakdown is as follows:

- pytorch_bridge: This folder represents the first (and only) point of contact with Poptorch. PopTorch calls these functions to create the IR, compile, and execute the poplar graph without directly being exposed to either MLIR, LLVM, or any poplar stuff directly. Most of the API is autogenerated by the builtin tablegen in the dialect or using scripts on those tablegen intermediaries.

- dialect : This is where we implement the IR. For operations we provide a tablegen file which implements a given operation and via TableGen and using our own scripts we build up all of the boilerplate code required implement and expose and operation to Poptorch from those tablegen intermediaries. This folder also defines the dialect and any type, traits, or interfaces used by the IR.

- lower_to_poplar : Provides the poplar/poplibs based implementations of operations and the boilerplate required to compile and execute a poplar graph.

- scripts : Some helper scripts to automatically generate the poptorch facing API from the tablegen tool json output.


Pytorch bridge is built into poptorch_compiler.so and the other two are built as static libraries. It is sometimes useful to build them in isolation which can be done via any of:

```
ninja poptorch_dialect
ninja lower_to_poplar
ninja poptorch_compiler
```

The script will run automatically when the dialect is built.

# Adding a new IR node.

To add a new IR node you just add it to the tablegen. Once it is in the tablegen all the IR and poptorch API boiler plate will be generated for you automatically.

```
def Poptorch_zero_: Poptorch_Op<"zero_", []> {
    let arguments = (ins Poptorch_tensor:$input);
}
```

If the tabelgen `def` inherits from `Poptorch_Op` (directly or indirecty) it will be automatically picked up and the lower to poplar section expects a function with the following signiture to be provided:

```
void NAME_OF_MY_OPERATION::lowerToPoplar(CompilerContext &context)
```

Not providing this will result in a link error. If the operation does not have a poplar implementation make it inherit from `Poptorch_AbstractOp`

For the zero operation above the code is:

```
void zero_::lowerToPoplar(CompilerContext &context) {
  poplar::Tensor input = context.fromSsa(this->input()); // Turn the tablegen 'input' into a poplar tensor.
  popops::zero(context.graph, input, context.seq); // Apply the zero operation.
}
```


# Autogenerated PopART API

Most of the PopART API has already been turned into a stub Tablegen node with the function definition and builder defined for you. To add one of these functions copy it from "dialect/include/dialect/ops/Generated.td" into a suitable tablegen file. These actually already get compiled and linked up, however the implementations are just stubs in lower_to_poplar/ops/Generated.cpp.