New features:
- Added support for torch.nn.InstanceNorm1d, torch.nn.InstanceNorm2d and torch.nn.InstanceNorm3d
- Fixed issue with torch.nn.GroupNorm where only 4-dimensional inputs could be used
- Replaced Adam with AdamW optimizer.
- Added support for the following loss functions:
    torch.nn.KLDivLoss, torch.nn.PoissonNLLLoss, torch.nn.HingeEmbeddingLoss, torch.nn.BCEWithLogitsLoss, torch.nn.SmoothL1Loss, torch.nn.SoftMarginLoss, torch.nn.CosineEmbeddingLoss, torch.nn.MarginRankingLoss, torch.nn.TripletMarginLoss
    torch.nn.NLLLoss for aten::nll_loss2d
- Added support for torch.optim.RMSprop optimizer
- Added support for bool inputs to models
- Improved support for half type models and inputs.
    Using a mix of float 16 and float 32 inputs is now supported. Please see
    the documentation for cases in which a model might use different types
    compared to when run natively with PyTorch.
- Added support for serialized matrix multiplications
(poptorch.serializedMatMul)
- Add support for POPTORCH_IPU_MODEL_VERSION environment variable.
- Added support for torch.cumsum
- Add support for pipelined / phased / sharded execution.
- Add PoplarExecutor.compile() to compile the model without executing it.
- Use sphinx-build to generate the documentation.
- Use Miniconda as build environment.
- Added support for torch.meshgrid
- Added support for torch.cartesian_prod
- Optimized torch.matmul implementation with limitations
    Fused its input 0's batch dimensions with the row dimension
    to avoid ReduceSum in its backward pass, for performance purpose
- Added partial support for torch.einsum
    Diagonals and ellipsis notation is unsupported
- Add support for executable caching: poptorch.Options.enableExecutableCaching()
- Add optional title argument to poptorch.ipu_print_tensor
- Add len() method to poptorch.AsynchronousDataLoader
- Added support for LAMB optimizer
- Add support for recomputationCheckpoint()
- Added support for torch.tensordot
- Add support for rounding up the number of IPU used to allow models which
  specify of number of IPUs which is not a power of 2:
  poptorch.Options.autoRoundNumIPUs(True) NB, this will reserve but not use IPUs
  and so it is preferable to specify the model to use a number of IPUs which is
  a power of two
- Optimized torch.matmul implementation with limitations
    Fused its input 0's batch dimensions with the row dimension
    to avoid ReduceSum in its backward pass, for performance purpose
- Added support for multi-convolutions with poptorch.MultiConv
